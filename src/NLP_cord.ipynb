{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used to draft a sample NLP pipeline with CORD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample structure\n",
    "\n",
    "Loader => Sentencer => NER => "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the whole CORD dataset should be loaded set exc_cord_loader as True\n",
    "# If the downloader script should be run to get a set of articles with PMID, set False\n",
    "exc_cord_loader = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cord Loader script creates a JSON file from the metadata.csv file from the CORD-19 dataset of journals\n",
    "The dataset can be found at: https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exc_cord_loader == True:\n",
    "    from scripts import cord_loader\n",
    "\n",
    "    cord_loader_input = '../data/cord/metadata.csv'\n",
    "    cord_loader_output = '../data/cord/metadata.json'\n",
    "\n",
    "    cord_loader.run(cord_loader_input, cord_loader_output)\n",
    "    print('fin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cord loader seems to execute without issue on the latest version of CORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, the downloader script uses a text file with PUBMED IDs, and loads those articles into a JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving batch 1...\n",
      "Saved 4/5 articles so far.\n",
      "\n",
      "Downloading and saving batch 2...\n",
      "Saved 5/5 articles so far.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if exc_cord_loader == False:\n",
    "    from scripts import downloader\n",
    "    \n",
    "    downloader_input = \"../data/example_pmid_list.txt\"\n",
    "    downloader_output = \"../data/example_pmid_list.json\"\n",
    "    downloader_batch_size = 4\n",
    "    \n",
    "    downloader.run(\n",
    "    input_file=downloader_input,\n",
    "    output_file=downloader_output,\n",
    "    batch_size=downloader_batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We run the sentencer script based on the JSON files.\n",
    "As a result, we get a utf8 encoded json file with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc636ce387ed4e67b8c188dc4f433c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=279304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts import splitter\n",
    "\n",
    "sentencer_input = \"../data/cord/metadata.json\"\n",
    "sentencer_output = \"../data/cord/metadata-sentences.json\"\n",
    "\n",
    "# Load the metadata.json file with abstracts\n",
    "with open(sentencer_input, \"r\", encoding='utf8') as f:\n",
    "    full_articles = json.loads(f.read())\n",
    "\n",
    "articles = {}\n",
    "\n",
    "# create sentences using the splitter script\n",
    "for id, article in tqdm(full_articles.items()):\n",
    "    articles[id] = {\n",
    "        # **articles[id], # include other fields\n",
    "        \"title\": article[\"title\"],\n",
    "        \"sentences\": list(map(\n",
    "            lambda sentence: {\"text\": sentence},\n",
    "            splitter.split_into_sentences(article[\"abstract\"])\n",
    "        ))\n",
    "    }\n",
    "\n",
    "# Notice the change of encoding utf8\n",
    "with open(sentencer_output, \"w\",encoding='utf8') as f:\n",
    "    f.write(json.dumps(articles, indent=2, ensure_ascii=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Named Entity Recognition with the help of BioBERT\n",
    "Download the BioBERT model from: https://drive.google.com/drive/folders/1neThCq4MqFPd0133WDDC4MYUycE84fT7?usp=sharing\n",
    "Save the model in models directory. This model has been fine tuned on BC5CDR-chem dataset\n",
    "\n",
    "Make sure to install tf2onnx by \n",
    "\n",
    "```\n",
    "pip install -U tf2onnx\n",
    "pip install onnxruntime\n",
    "pip install transformers\n",
    "pip install \n",
    "```\n",
    "\n",
    "For fine tuning and training BERT, I need to look at the utils/chemprot/bert_finetune.py code in depth\n",
    "\n",
    "Note: changed util f.read to utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating NER session...\n",
      "Loading model:\n",
      "  ../models/biobert/biobert_ner.onnx\n",
      "Model loaded succesfully\n",
      "\n",
      "Created NER session.\n",
      "Limiting NER to 2 articles.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db3d9810a2643b5ac36f2f7e2cd62e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished running NER script.\n"
     ]
    }
   ],
   "source": [
    "from scripts import util\n",
    "from scripts.ner_inference import NERInferenceSession\n",
    "from scripts.entity_parser import co_occurrence_extractor, detokenize\n",
    "\n",
    "\n",
    "ner_input = \"../data/cord/metadata-sentences.json\"\n",
    "ner_output = \"../data/cord/metadata-ner-done.json\"\n",
    "model_dir = \"../models/biobert/\"\n",
    "model_name = \"biobert_ner.onnx\"\n",
    "model_vocab = \"vocab.txt\"\n",
    "labels = [\"[PAD]\", \"B\", \"I\", \"O\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "clear_old_results = True\n",
    "article_limit = 2\n",
    "\n",
    "with open(ner_input, \"r\", encoding='utf8') as f:\n",
    "    articles = json.loads(f.read())\n",
    "\n",
    "print(\"Creating NER session...\")\n",
    "ner_session = NERInferenceSession(\n",
    "    model_dir=model_dir,\n",
    "    model_name=model_name,\n",
    "    model_vocab=model_vocab,\n",
    "    labels=labels,\n",
    ")\n",
    "print(\"Created NER session.\")\n",
    "\n",
    "# For experimentation: limit number of articles to process (and to output)\n",
    "limit = article_limit\n",
    "if limit > 0:\n",
    "    print(f\"Limiting NER to {limit} articles.\")\n",
    "    a = {}\n",
    "    i = 0\n",
    "    for id in articles:\n",
    "        if i >= limit:\n",
    "            break\n",
    "        a[id] = articles[id]\n",
    "        i += 1\n",
    "    articles = a\n",
    "\n",
    "#clear old results\n",
    "if clear_old_results==True:\n",
    "    try:\n",
    "        os.remove(ner_output)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "# Becuase we want to save the result periodically.\n",
    "batch_index = 0\n",
    "batch_size = 2\n",
    "\n",
    "# Run prediction on each sentence in each article.\n",
    "for pmid in tqdm(articles):\n",
    "    if batch_index > batch_size:\n",
    "        util.append_to_json_file(ner_output, articles)\n",
    "        batch_index = 0\n",
    "    sentences = articles[pmid][\"sentences\"]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        token_label_pairs = ner_session.predict(sentence[\"text\"])\n",
    "        x = co_occurrence_extractor(detokenize(token_label_pairs))\n",
    "        articles[pmid][\"sentences\"][i][\"entities\"] = x[\"entities\"]\n",
    "        articles[pmid][\"sentences\"][i][\"text_new\"] = x[\"text\"]\n",
    "    batch_index += 1\n",
    "\n",
    "util.append_to_json_file(ner_output, articles)\n",
    "\n",
    "print(\"Finished running NER script.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship extraction code, even though described by the authors, was not available in the repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the analysis code from the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ft2font' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-98c697583cbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0manalysis_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/metadata-ner-done.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# analysis_output = '../data/metadata-analysis-'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalysis_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AITSLAB\\nlp pipeline\\rafsan\\src\\scripts\\analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[0m_check_versions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m_check_versions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m# Quickfix to ensure Microsoft Visual C++ redistributable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;31m# DLLs are loaded before importing kiwisolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mft2font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     for modname, minver in [\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ft2font' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from scripts import analysis\n",
    "\n",
    "analysis_input = \"../data/metadata-ner-done.json\"\n",
    "# analysis_output = '../data/metadata-analysis-'\n",
    "with open(analysis_config[\"input_path\"], \"r\") as f:\n",
    "    articles = json.loads(f.read())\n",
    "\n",
    "analysis.run(articles, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:\n",
      "  ../models/biobert/biobert_ner.onnx\n",
      "Model loaded succesfully\n",
      "\n",
      " - - - Gold standard metrics - - -\n",
      "Label count:\n",
      "\tO label count: 823456\n",
      "\tB label count: 29486\n",
      "\tI label count: 34863\n",
      "\n",
      "Occurrence count:\n",
      "\t0_occurrence count: 16071\n",
      "\t1_occurrence count: 7286\n",
      "\t2_occurrence count: 3893\n",
      "\t3_occurrence count: 1629\n",
      "\t4_occurrence count: 780\n",
      "\t5_occurrence count: 445\n",
      "\t6_occurrence count: 216\n",
      "\t7_occurrence count: 108\n",
      "\t8_occurrence count: 81\n",
      "\t9_occurrence count: 43\n",
      "\t10_occurrence count: 29\n",
      "\t11_occurrence count: 19\n",
      "\t12_occurrence count: 8\n",
      "\t13_occurrence count: 11\n",
      "\t14_occurrence count: 5\n",
      "\t15_occurrence count: 5\n",
      "\t16_occurrence count: 5\n",
      "\t20_occurrence count: 2\n",
      "\t25_occurrence count: 1\n",
      "\t29_occurrence count: 1\n",
      "\t38_occurrence count: 1\n",
      " - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running over 30639 sentences\n",
      "Predicted 1/30639 sentences so far.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 3/30639 sentences so far.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 882816 tokens with 29277 phrases; found: 24917 phrases; correct: 17417.\n",
      "accuracy:  96.76%; precision:  69.90%; recall:  59.49%; FB1:  64.28\n",
      "\n",
      "Confusion matrix:\n",
      "{'true_negative': 813027, 'true_positive': 42216, 'false_positive': 6867, 'false_negative': 20601}\n",
      "Recall: 0.6720473757103969\n",
      "Precision: 0.8600941262759\n",
      "\n",
      "Token matrix:\n",
      "{'O': defaultdict(<class 'int'>, {'O': 813027, 'B': 4301, 'I': 2566, '[SEP]': 100, '[PAD]': 4}), 'B': defaultdict(<class 'int'>, {'B': 18561, 'O': 10325, 'I': 390, '[SEP]': 1}), 'I': defaultdict(<class 'int'>, {'I': 22601, 'O': 10276, 'B': 664})}\n",
      "\n",
      " - - - Gold standard metrics - - -\n",
      "Label count:\n",
      "\tO label count: 712648\n",
      "\tB label count: 25346\n",
      "\tI label count: 29642\n",
      "\n",
      "Occurrence count:\n",
      "\t0_occurrence count: 13936\n",
      "\t1_occurrence count: 6141\n",
      "\t2_occurrence count: 3276\n",
      "\t3_occurrence count: 1451\n",
      "\t4_occurrence count: 749\n",
      "\t5_occurrence count: 363\n",
      "\t6_occurrence count: 181\n",
      "\t7_occurrence count: 101\n",
      "\t8_occurrence count: 56\n",
      "\t9_occurrence count: 37\n",
      "\t10_occurrence count: 22\n",
      "\t11_occurrence count: 16\n",
      "\t12_occurrence count: 12\n",
      "\t13_occurrence count: 3\n",
      "\t14_occurrence count: 5\n",
      "\t15_occurrence count: 3\n",
      "\t16_occurrence count: 3\n",
      "\t17_occurrence count: 4\n",
      "\t18_occurrence count: 4\n",
      "\t33_occurrence count: 1\n",
      " - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running over 26364 sentences\n",
      "processed 761812 tokens with 25114 phrases; found: 20726 phrases; correct: 14852.\n",
      "accuracy:  96.89%; precision:  71.66%; recall:  59.14%; FB1:  64.80\n",
      "\n",
      "Confusion matrix:\n",
      "{'true_negative': 703600, 'true_positive': 35336, 'false_positive': 5300, 'false_negative': 17492}\n",
      "Recall: 0.6688877110623155\n",
      "Precision: 0.8695737769465498\n",
      "\n",
      "Token matrix:\n",
      "{'O': defaultdict(<class 'int'>, {'O': 703600, 'B': 3340, 'I': 1960, '[SEP]': 81, '[PAD]': 2}), 'B': defaultdict(<class 'int'>, {'B': 15843, 'O': 8951, 'I': 319, '[SEP]': 1}), 'I': defaultdict(<class 'int'>, {'I': 18666, 'O': 8541, 'B': 508})}\n",
      "\n",
      " - - - Gold standard metrics - - -\n",
      "Label count:\n",
      "\tO label count: 828284\n",
      "\tB label count: 29478\n",
      "\tI label count: 35923\n",
      "\n",
      "Occurrence count:\n",
      "\t0_occurrence count: 16175\n",
      "\t1_occurrence count: 7165\n",
      "\t2_occurrence count: 3824\n",
      "\t3_occurrence count: 1742\n",
      "\t4_occurrence count: 827\n",
      "\t5_occurrence count: 411\n",
      "\t6_occurrence count: 231\n",
      "\t7_occurrence count: 105\n",
      "\t8_occurrence count: 85\n",
      "\t9_occurrence count: 56\n",
      "\t10_occurrence count: 16\n",
      "\t11_occurrence count: 15\n",
      "\t12_occurrence count: 11\n",
      "\t13_occurrence count: 7\n",
      "\t14_occurrence count: 5\n",
      "\t17_occurrence count: 1\n",
      "\t18_occurrence count: 3\n",
      "\t26_occurrence count: 1\n",
      "\t27_occurrence count: 1\n",
      "\t29_occurrence count: 1\n",
      " - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running over 30682 sentences\n",
      "processed 889037 tokens with 29279 phrases; found: 24665 phrases; correct: 17057.\n",
      "accuracy:  96.60%; precision:  69.15%; recall:  58.26%; FB1:  63.24\n",
      "\n",
      "Confusion matrix:\n",
      "{'true_negative': 818198, 'true_positive': 41711, 'false_positive': 7367, 'false_negative': 21669}\n",
      "Recall: 0.6581098138213948\n",
      "Precision: 0.8498920086393088\n",
      "\n",
      "Token matrix:\n",
      "{'O': defaultdict(<class 'int'>, {'O': 818198, 'B': 4529, 'I': 2838, '[SEP]': 85, '[PAD]': 7}), 'B': defaultdict(<class 'int'>, {'B': 18211, 'O': 10613, 'I': 455}), 'I': defaultdict(<class 'int'>, {'I': 22442, 'O': 11056, 'B': 603})}\n",
      "\n",
      " - - - Gold standard metrics - - -\n",
      "Label count:\n",
      "\tO label count: 1651740\n",
      "\tB label count: 58964\n",
      "\tI label count: 70786\n",
      "\n",
      "Occurrence count:\n",
      "\t0_occurrence count: 32246\n",
      "\t1_occurrence count: 14451\n",
      "\t2_occurrence count: 7717\n",
      "\t3_occurrence count: 3371\n",
      "\t4_occurrence count: 1607\n",
      "\t5_occurrence count: 856\n",
      "\t6_occurrence count: 447\n",
      "\t7_occurrence count: 213\n",
      "\t8_occurrence count: 166\n",
      "\t9_occurrence count: 99\n",
      "\t10_occurrence count: 45\n",
      "\t11_occurrence count: 34\n",
      "\t12_occurrence count: 19\n",
      "\t13_occurrence count: 18\n",
      "\t14_occurrence count: 10\n",
      "\t15_occurrence count: 5\n",
      "\t16_occurrence count: 5\n",
      "\t17_occurrence count: 1\n",
      "\t18_occurrence count: 3\n",
      "\t20_occurrence count: 2\n",
      "\t25_occurrence count: 1\n",
      "\t26_occurrence count: 1\n",
      "\t27_occurrence count: 1\n",
      "\t29_occurrence count: 2\n",
      "\t38_occurrence count: 1\n",
      " - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running over 61321 sentences\n",
      "processed 1771853 tokens with 58556 phrases; found: 49582 phrases; correct: 34474.\n",
      "accuracy:  96.68%; precision:  69.53%; recall:  58.87%; FB1:  63.76\n",
      "\n",
      "Confusion matrix:\n",
      "{'true_negative': 1631225, 'true_positive': 83927, 'false_positive': 14234, 'false_negative': 42270}\n",
      "Recall: 0.6650475050912462\n",
      "Precision: 0.8549933272888418\n",
      "\n",
      "Token matrix:\n",
      "{'O': defaultdict(<class 'int'>, {'O': 1631225, 'B': 8830, 'I': 5404, '[SEP]': 185, '[PAD]': 11}), 'B': defaultdict(<class 'int'>, {'B': 36772, 'O': 20938, 'I': 845, '[SEP]': 1}), 'I': defaultdict(<class 'int'>, {'I': 45043, 'O': 21332, 'B': 1267})}\n",
      "\n",
      "Finished running metrics script.\n"
     ]
    }
   ],
   "source": [
    "from scripts import metrics\n",
    "from scripts.ner_inference import NERInferenceSession\n",
    "from scripts.entity_parser import co_occurrence_extractor, detokenize\n",
    "\n",
    "gold_standard_path = '../data/NER_data/BC4CHEMD/'#\"../data/gold-standard/BC4CHEMD/\"\n",
    "metrics_output_path = \"../data/metrics_BC4CHEMD.json\"\n",
    "biobert_path = \"../models/biobert\"\n",
    "biobert_metrics = True\n",
    "bilstm_metrics = True\n",
    "co_occurrence_metrics = True\n",
    "\n",
    "# ner_input = \"../data/cord/metadata-sentences.json\"\n",
    "# ner_output = \"../data/cord/metadata-ner-done.json\"\n",
    "model_dir = \"../models/biobert/\"\n",
    "model_name = \"biobert_ner.onnx\"\n",
    "model_vocab = \"vocab.txt\"\n",
    "labels = [\"[PAD]\", \"B\", \"I\", \"O\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "# clear_old_results = True\n",
    "# article_limit = 100\n",
    "\n",
    "ner_session = NERInferenceSession(\n",
    "        model_dir=model_dir,\n",
    "        model_name=model_name,\n",
    "        model_vocab=model_vocab,\n",
    "        labels=labels,\n",
    ")\n",
    "\n",
    "dir = gold_standard_path\n",
    "\n",
    "open(metrics_output_path, \"w\").close()\n",
    "\n",
    "files = [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
    "for file in files:\n",
    "    with open(metrics_output_path, \"a+\") as out_f:\n",
    "        out_f.write(\"\\n\\n\" + \"-\"*10 + file + \"-\"*10)\n",
    "    metrics.gs_metrics(dir + file)\n",
    "    metrics.biobert_metrics(ner_session, dir + file, metrics_output_path)\n",
    "\n",
    "print(\"Finished running metrics script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x1ffb0749610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2060'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a32920475ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\u001b[0m in \u001b[0;36mgpu_device_name\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[1;34m\"\"\"Returns the name of a GPU device if available or the empty string.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[1;34m(session_config)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mserialized_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   return [\n\u001b[1;32m---> 43\u001b[1;33m       \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pywrap_device_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m   ]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0dd9ecd7b9ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mdevices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcabc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcabc abc xyz\n",
      "['', '', '']\n",
      "abc\n",
      "abcxyz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
