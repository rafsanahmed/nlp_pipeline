{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used to draft a sample NLP pipeline with CORD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample structure\n",
    "\n",
    "Loader => Sentencer => NER => "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the whole CORD dataset should be loaded set exc_cord_loader as True\n",
    "# If the downloader script should be run to get a set of articles with PMID, set False\n",
    "exc_cord_loader = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cord Loader script creates a JSON file from the metadata.csv file from the CORD-19 dataset of journals\n",
    "The dataset can be found at: https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exc_cord_loader == True:\n",
    "    from scripts import cord_loader\n",
    "\n",
    "    cord_loader_input = '../data/cord/metadata.csv'\n",
    "    cord_loader_output = '../data/cord/metadata.json'\n",
    "\n",
    "    cord_loader.run(cord_loader_input, cord_loader_output)\n",
    "    print('fin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cord loader seems to execute without issue on the latest version of CORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, the downloader script uses a text file with PUBMED IDs, and loads those articles into a JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving batch 1...\n",
      "Saved 4/5 articles so far.\n",
      "\n",
      "Downloading and saving batch 2...\n",
      "Saved 5/5 articles so far.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if exc_cord_loader == False:\n",
    "    from scripts import downloader\n",
    "    \n",
    "    downloader_input = \"../data/example_pmid_list.txt\"\n",
    "    downloader_output = \"../data/example_pmid_list.json\"\n",
    "    downloader_batch_size = 4\n",
    "    \n",
    "    downloader.run(\n",
    "    input_file=downloader_input,\n",
    "    output_file=downloader_output,\n",
    "    batch_size=downloader_batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We run the sentencer script based on the JSON files.\n",
    "As a result, we get a utf8 encoded json file with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc636ce387ed4e67b8c188dc4f433c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=279304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts import splitter\n",
    "\n",
    "sentencer_input = \"../data/cord/metadata.json\"\n",
    "sentencer_output = \"../data/cord/metadata-sentences.json\"\n",
    "\n",
    "# Load the metadata.json file with abstracts\n",
    "with open(sentencer_input, \"r\", encoding='utf8') as f:\n",
    "    full_articles = json.loads(f.read())\n",
    "\n",
    "articles = {}\n",
    "\n",
    "# create sentences using the splitter script\n",
    "for id, article in tqdm(full_articles.items()):\n",
    "    articles[id] = {\n",
    "        # **articles[id], # include other fields\n",
    "        \"title\": article[\"title\"],\n",
    "        \"sentences\": list(map(\n",
    "            lambda sentence: {\"text\": sentence},\n",
    "            splitter.split_into_sentences(article[\"abstract\"])\n",
    "        ))\n",
    "    }\n",
    "\n",
    "# Notice the change of encoding utf8\n",
    "with open(sentencer_output, \"w\",encoding='utf8') as f:\n",
    "    f.write(json.dumps(articles, indent=2, ensure_ascii=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Named Entity Recognition with the help of BioBERT\n",
    "Download the BioBERT model from: https://drive.google.com/drive/folders/1neThCq4MqFPd0133WDDC4MYUycE84fT7?usp=sharing\n",
    "Save the model in models directory. This model has been fine tuned on BC5CDR-chem dataset\n",
    "\n",
    "Make sure to install tf2onnx by \n",
    "\n",
    "```\n",
    "pip install -U tf2onnx\n",
    "pip install onnxruntime\n",
    "pip install transformers\n",
    "pip install \n",
    "```\n",
    "\n",
    "For fine tuning and training BERT, I need to look at the utils/chemprot/bert_finetune.py code in depth\n",
    "\n",
    "Note: changed util f.read to utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating NER session...\n",
      "Loading model:\n",
      "  ../models/biobert/biobert_ner.onnx\n",
      "Model loaded succesfully\n",
      "\n",
      "Created NER session.\n",
      "Limiting NER to 2 articles.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db3d9810a2643b5ac36f2f7e2cd62e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished running NER script.\n"
     ]
    }
   ],
   "source": [
    "from scripts import util\n",
    "from scripts.ner_inference import NERInferenceSession\n",
    "from scripts.entity_parser import co_occurrence_extractor, detokenize\n",
    "\n",
    "\n",
    "ner_input = \"../data/cord/metadata-sentences.json\"\n",
    "ner_output = \"../data/cord/metadata-ner-done.json\"\n",
    "model_dir = \"../models/biobert/\"\n",
    "model_name = \"biobert_ner.onnx\"\n",
    "model_vocab = \"vocab.txt\"\n",
    "labels = [\"[PAD]\", \"B\", \"I\", \"O\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "clear_old_results = True\n",
    "article_limit = 2\n",
    "\n",
    "with open(ner_input, \"r\", encoding='utf8') as f:\n",
    "    articles = json.loads(f.read())\n",
    "\n",
    "print(\"Creating NER session...\")\n",
    "ner_session = NERInferenceSession(\n",
    "    model_dir=model_dir,\n",
    "    model_name=model_name,\n",
    "    model_vocab=model_vocab,\n",
    "    labels=labels,\n",
    ")\n",
    "print(\"Created NER session.\")\n",
    "\n",
    "# For experimentation: limit number of articles to process (and to output)\n",
    "limit = article_limit\n",
    "if limit > 0:\n",
    "    print(f\"Limiting NER to {limit} articles.\")\n",
    "    a = {}\n",
    "    i = 0\n",
    "    for id in articles:\n",
    "        if i >= limit:\n",
    "            break\n",
    "        a[id] = articles[id]\n",
    "        i += 1\n",
    "    articles = a\n",
    "\n",
    "#clear old results\n",
    "if clear_old_results==True:\n",
    "    try:\n",
    "        os.remove(ner_output)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "# Becuase we want to save the result periodically.\n",
    "batch_index = 0\n",
    "batch_size = 2\n",
    "\n",
    "# Run prediction on each sentence in each article.\n",
    "for pmid in tqdm(articles):\n",
    "    if batch_index > batch_size:\n",
    "        util.append_to_json_file(ner_output, articles)\n",
    "        batch_index = 0\n",
    "    sentences = articles[pmid][\"sentences\"]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        token_label_pairs = ner_session.predict(sentence[\"text\"])\n",
    "        x = co_occurrence_extractor(detokenize(token_label_pairs))\n",
    "        articles[pmid][\"sentences\"][i][\"entities\"] = x[\"entities\"]\n",
    "        articles[pmid][\"sentences\"][i][\"text_new\"] = x[\"text\"]\n",
    "    batch_index += 1\n",
    "\n",
    "util.append_to_json_file(ner_output, articles)\n",
    "\n",
    "print(\"Finished running NER script.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship extraction code, even though described by the authors, was not available in the repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the analysis code from the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ft2font' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-98c697583cbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0manalysis_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/metadata-ner-done.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# analysis_output = '../data/metadata-analysis-'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalysis_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AITSLAB\\nlp pipeline\\rafsan\\src\\scripts\\analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[0m_check_versions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m_check_versions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m# Quickfix to ensure Microsoft Visual C++ redistributable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;31m# DLLs are loaded before importing kiwisolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mft2font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     for modname, minver in [\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ft2font' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\rafsa\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from scripts import analysis\n",
    "\n",
    "analysis_input = \"../data/metadata-ner-done.json\"\n",
    "# analysis_output = '../data/metadata-analysis-'\n",
    "with open(analysis_config[\"input_path\"], \"r\") as f:\n",
    "    articles = json.loads(f.read())\n",
    "\n",
    "analysis.run(articles, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:\n",
      "  ../models/biobert/biobert_ner.onnx\n",
      "Model loaded succesfully\n",
      "\n",
      " - - - Gold standard metrics - - -\n",
      "Label count:\n",
      "\tO label count: 823456\n",
      "\tB label count: 29486\n",
      "\tI label count: 34863\n",
      "\n",
      "Occurrence count:\n",
      "\t0_occurrence count: 16071\n",
      "\t1_occurrence count: 7286\n",
      "\t2_occurrence count: 3893\n",
      "\t3_occurrence count: 1629\n",
      "\t4_occurrence count: 780\n",
      "\t5_occurrence count: 445\n",
      "\t6_occurrence count: 216\n",
      "\t7_occurrence count: 108\n",
      "\t8_occurrence count: 81\n",
      "\t9_occurrence count: 43\n",
      "\t10_occurrence count: 29\n",
      "\t11_occurrence count: 19\n",
      "\t12_occurrence count: 8\n",
      "\t13_occurrence count: 11\n",
      "\t14_occurrence count: 5\n",
      "\t15_occurrence count: 5\n",
      "\t16_occurrence count: 5\n",
      "\t20_occurrence count: 2\n",
      "\t25_occurrence count: 1\n",
      "\t29_occurrence count: 1\n",
      "\t38_occurrence count: 1\n",
      " - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running over 30639 sentences\n",
      "Predicted 1/30639 sentences so far.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 6742/30639 sentences so far.\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-111c61f925c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mout_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgs_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiobert_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mner_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_output_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished running metrics script.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AITSLAB\\nlp pipeline\\rafsan\\src\\scripts\\metrics.py\u001b[0m in \u001b[0;36mbiobert_metrics\u001b[1;34m(model, input_path, output_path)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mpred_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AITSLAB\\nlp pipeline\\rafsan\\src\\scripts\\ner_inference.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         _, logits, _ = self.session.run([], {\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;34m\"segment_ids_1:0\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token_type_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;34m\"input_mask_1_raw_output___9:0\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scripts import metrics\n",
    "from scripts.ner_inference import NERInferenceSession\n",
    "from scripts.entity_parser import co_occurrence_extractor, detokenize\n",
    "\n",
    "gold_standard_path = '../data/NER_data/BC4CHEMD/'#\"../data/gold-standard/BC4CHEMD/\"\n",
    "metrics_output_path = \"../data/metrics_BC4CHEMD.json\"\n",
    "biobert_path = \"../models/biobert\"\n",
    "biobert_metrics = True\n",
    "bilstm_metrics = True\n",
    "co_occurrence_metrics = True\n",
    "\n",
    "# ner_input = \"../data/cord/metadata-sentences.json\"\n",
    "# ner_output = \"../data/cord/metadata-ner-done.json\"\n",
    "model_dir = \"../models/biobert/\"\n",
    "model_name = \"biobert_ner.onnx\"\n",
    "model_vocab = \"vocab.txt\"\n",
    "labels = [\"[PAD]\", \"B\", \"I\", \"O\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "# clear_old_results = True\n",
    "# article_limit = 100\n",
    "\n",
    "ner_session = NERInferenceSession(\n",
    "        model_dir=model_dir,\n",
    "        model_name=model_name,\n",
    "        model_vocab=model_vocab,\n",
    "        labels=labels,\n",
    ")\n",
    "\n",
    "dir = gold_standard_path\n",
    "\n",
    "open(metrics_output_path, \"w\").close()\n",
    "\n",
    "files = [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
    "for file in files:\n",
    "    with open(metrics_output_path, \"a+\") as out_f:\n",
    "        out_f.write(\"\\n\\n\" + \"-\"*10 + file + \"-\"*10)\n",
    "    metrics.gs_metrics(dir + file)\n",
    "    metrics.biobert_metrics(ner_session, dir + file, metrics_output_path)\n",
    "\n",
    "print(\"Finished running metrics script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x1ffb0749610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2060'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a32920475ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\u001b[0m in \u001b[0;36mgpu_device_name\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[1;34m\"\"\"Returns the name of a GPU device if available or the empty string.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[1;34m(session_config)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mserialized_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   return [\n\u001b[1;32m---> 43\u001b[1;33m       \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pywrap_device_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m   ]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0dd9ecd7b9ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mdevices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcabc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcabc abc xyz\n",
      "['', '', '']\n",
      "abc\n",
      "abcxyz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
